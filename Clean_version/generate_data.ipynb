{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541d48d2",
   "metadata": {},
   "source": [
    "## Multiplayer Proxy Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import module\n",
    "import numpy as np\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb8f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14acfd41",
   "metadata": {},
   "source": [
    "## Useful functions\n",
    "\n",
    "Comparatively to the last one: goal is always the same, no inverse, actions are always the same, steps after discard is always the same, all parameters are in the ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de570cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_proxy_nodes_from_esn(esn, trials=10):  \n",
    "    \"\"\"\n",
    "    Compute proxy nodes from an Echo State Network (ESN) by running trials with random agent actions.\n",
    "    The function calculates the average values of the goal node and the proxy nodes based on the ESN's states.\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = esn.n\n",
    "    possible_nodes =  [i for i in range(num_nodes) if i not in esn.action_nodes and i not in esn.goal]\n",
    "    run_data_base = np.zeros((trials, num_nodes)) \n",
    "    avg_goal_values = []\n",
    "    avg_proxy_values = []\n",
    "    for k in range(trials):\n",
    "        # Generate random agent actions\n",
    "        action_node_values = np.random.choice([-1.0, 0.0, 1.0], size=len(esn.action_nodes))\n",
    "        \n",
    "        # Run the ESN with the random agent actions\n",
    "        states = esn.run( agent_values=action_node_values)\n",
    "        data_run =  np.mean(states[esn.proxy_discard: esn.proxy_discard + esn.measure_time, :], axis=0)\n",
    "        data_run[esn.goal] = np.mean(states[esn.goal_discard: esn.goal_discard+ esn.measure_time, esn.goal], axis=0)\n",
    "        run_data_base[k] = data_run\n",
    "        # Compute the average values \n",
    "        avg_goal_values.append(np.mean(states[:, esn.goal], axis=0))\n",
    "\n",
    "    correlations = np.mean(np.corrcoef(run_data_base, rowvar= False)[:, esn.goal], axis=1)#here the runs are on axis 0 and the nodes on axis 1\n",
    "\n",
    "    bin_edges = np.linspace(-1, 1, 100)\n",
    "    bin_indices =  np.histogram(correlations, bin_edges)[0] \n",
    "    bin_indices = [int(k) for k in  bin_indices]\n",
    "    # check if there are nan values in correlations\n",
    "    if np.isnan(correlations).any():\n",
    "        raise ValueError(\"NaN values found in correlations\")\n",
    "        \n",
    "\n",
    "    # Sort nodes by correlation in descending order and select top nodes\n",
    "    sorted_indices = [k for k in np.argsort(correlations)[::-1 ] if k in possible_nodes]\n",
    "    proxy_nodes = sorted_indices[:1]\n",
    "    avg_proxy_values.append(np.mean(run_data_base[:, proxy_nodes], axis=0))\n",
    "    goal_value = np.mean(avg_goal_values)\n",
    "    proxy_value = np.mean(avg_proxy_values)\n",
    "    print(bin_indices)\n",
    "\n",
    "    return proxy_nodes, goal_value, proxy_value, correlations, bin_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccfc3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_node(esn,  trials, proxy_nodes, is_goal=False):\n",
    "\n",
    "    n = esn.n \n",
    "    num_action_nodes = len(esn.action_nodes)\n",
    "    start_time = esn.proxy_discard*(not is_goal) + esn.goal_discard*is_goal\n",
    "    end_time = start_time + esn.measure_time\n",
    "    # define function to test an agent action \n",
    "    def test_agent_action(agent_action, action_value):\n",
    "        \"\"\"\n",
    "        Test the average value of an action node if all other nodes are set to random values\n",
    "        \"\"\"\n",
    "        run_data_base = [0 for i in range(trials)]\n",
    "        proxy_samples    = np.asarray([])\n",
    "        goal_samples    = np.asarray([])\n",
    "        actionVALS = []\n",
    "        for j in range(trials): \n",
    "            action_node_values = np.random.choice([-1.0, 0.0, 1.0], size=len(esn.action_nodes))\n",
    "            action_node_values[agent_action] = action_value\n",
    "            # REMOVE actionVALS.append(action_node_values)\n",
    "            states = esn.run(agent_values=action_node_values)\n",
    "            proxy_samples = np.concat((proxy_samples,np.mean(states[start_time:end_time, proxy_nodes], axis=0)))\n",
    "            goal_samples = np.concat((goal_samples,np.mean(states[esn.goal_discard:esn.goal_discard+esn.measure_time, esn.goal], axis=0)))\n",
    "        try:\n",
    "            correl_node_goal = np.corrcoef(proxy_samples, goal_samples)[0, 1]\n",
    "        except RuntimeWarning:\n",
    "            correl_node_goal = -np.inf\n",
    "        return np.mean(proxy_samples), np.mean(goal_samples), correl_node_goal\n",
    "    max_proxy_value = -1\n",
    "    goal_value_on_max_proxy = -1\n",
    "    correlation_on_max_proxy = 0\n",
    "    for i in range(num_action_nodes):\n",
    "        for val in [-1.0, 1.0]:\n",
    "            proxy_value, goal_value, correl_node_goal = test_agent_action(i, val)\n",
    "            if proxy_value > max_proxy_value:\n",
    "                max_proxy_value = proxy_value\n",
    "                goal_value_on_max_proxy = goal_value\n",
    "                correlation_on_max_proxy = correl_node_goal\n",
    "\n",
    "    return (goal_value_on_max_proxy, max_proxy_value, correlation_on_max_proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6210b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_compute_proxy_failure(param_ESN):\n",
    "    esn = module.EchoStateNetwork(param_ESN['n'],  spectral_radius=param_ESN['spectral_radius'], alpha = param_ESN['alpha'], \n",
    "                                avg_number_of_edges=param_ESN['avg_number_of_edges'], proxy_discard=param_ESN['proxy_discard'],\n",
    "                                goal_discard=param_ESN['goal_discard'], measure_time=param_ESN['measure_time'])\n",
    "\n",
    "    proxy_nodes, goal_base, proxy_base, correlations, bin_indices = compute_proxy_nodes_from_esn(esn, trials=param_ESN['trials'])\n",
    "    correlation_base = np.mean(correlations[proxy_nodes])\n",
    "    correlation_std = np.std(correlations)\n",
    "    maxed_goal_value, maxed_proxy_value, correlation_on_max_proxy = optimize_node(esn,  param_ESN['trials'], proxy_nodes)\n",
    "    optimal_goal_value, optimal_proxy_value, correlation_on_optimal_proxy =  optimize_node(esn,  param_ESN['trials'], esn.goal)\n",
    "    to_return = {\n",
    "        'correlation_std': correlation_std,\n",
    "        'maxed_goal_value': maxed_goal_value,\n",
    "        'goal_base': goal_base,\n",
    "        'maxed_proxy_value': maxed_proxy_value,\n",
    "        'correlation_on_max_proxy': correlation_on_max_proxy,\n",
    "        'correlation_base': correlation_base,\n",
    "        'optimal_goal_value': optimal_goal_value,\n",
    "        'optimal_proxy_value': optimal_proxy_value,\n",
    "        'correlation_on_optimal_proxy': correlation_on_optimal_proxy,\n",
    "        'bin_indices': bin_indices,\n",
    "        'proxy_base': proxy_base\n",
    "    }\n",
    "    print(\"parallel\",to_return['bin_indices'])\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e88c04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_compute_correlation(param_exp,param_ESN):\n",
    "    esn = module.EchoStateNetwork(param_ESN['n'],  spectral_radius=param_ESN['spectral_radius'], alpha = param_ESN['alpha'], \n",
    "                                avg_number_of_edges=param_ESN['avg_number_of_edges'], discard=param_ESN['discard'])\n",
    "    param_exp['esn'] = esn\n",
    "    # Compute proxy nodes\n",
    "    proxy_nodes, goal_base, proxy_base, correlations = compute_proxy_nodes_from_esn(esn, trials=param_ESN['trials'])\n",
    "    correlations = list(correlations)\n",
    "    to_return = {\n",
    "        'correlations': correlations,\n",
    "    } \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396b573",
   "metadata": {},
   "source": [
    "## Main loop to generate experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55454be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experimental_data(filename, param_grid, number_of_instances, intention = 'a', skip_to = 0):\n",
    "    \n",
    "    # creates a list of the parameters that will vary\n",
    "    varying_params = [a for a in param_grid.keys() if len(param_grid[a]) > 1]\n",
    "    # Create a list of all combinations of parameters\n",
    "\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    print(\"param_combinations\", len(param_combinations))\n",
    "    # Create a list of parameter names\n",
    "    param_names = list(param_grid.keys())\n",
    "    # Loop through all combinations of parameters\n",
    "\n",
    "    param_range = range(len(param_combinations))\n",
    "    if skip_to > 0:\n",
    "        param_range = range(skip_to, skip_to + 1)\n",
    "        print(\"skipping to\", skip_to, \"out of\", len(param_combinations))\n",
    "\n",
    "    with open(filename,intention) as fd:\n",
    "        fieldnames = ['correlation_std', 'maxed_goal_value','goal_base','maxed_proxy_value',\n",
    "                      'correlation_on_max_proxy','correlation_base','optimal_goal_value','optimal_proxy_value','correlation_on_optimal_proxy','bin_indices','proxy_base'] \n",
    "        writer = csv.DictWriter(fd, fieldnames=param_names + fieldnames)\n",
    "        if intention == 'w':\n",
    "            writer.writeheader()\n",
    "        for k in param_range:\n",
    "\n",
    "            params = param_combinations[k]\n",
    "            # Create a dictionary of parameters\n",
    "            param_ESN = dict(zip(param_names, params))\n",
    "            param_ESN['trials'] = 50\n",
    "            name = 'ESN'+ \"\".join(['_'+a +'_'+ str(b) for (a,b) in list(zip(param_names, params)) if a in varying_params])\n",
    "            print(\"instance\", k+1, \"out of \", len(param_combinations), \"name\", name)\n",
    "            \n",
    "            delta_goal_values,delta_optimal_goals,delta_proxy_correlations,List_output = [], [], [], []\n",
    "            # run the parallel computation of proxy nodes\n",
    "            \n",
    "            List_output = Parallel(n_jobs=1, return_as='list')(\n",
    "                [delayed(parallel_compute_proxy_failure)(param_ESN) for _ in range(number_of_instances)]\n",
    "            )\n",
    "            print(\"List_output\", List_output)\n",
    "            for temp_dict in List_output:\n",
    "                writer.writerow(param_ESN | temp_dict)\n",
    "            fd.flush()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e8f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_combinations 320\n",
      "skipping to 27 out of 320\n",
      "instance 28 out of  320 name ESN_n_120_goal_discard_150_spectral_radius_2_avg_number_of_edges_8\n",
      "[37, 30, 41, 69, 26, 68, 90, 17, 73, 77, 33, 79, 16, 80, 70, 54, 73, 56, 68, 28, 49, 42, 22, 63, 36, 21, 52, 57, 70, 46, 29, 54, 17, 74, 18, 20, 13, 25, 20, 59, 31, 80, 23, 40, 70, 24, 38, 55, 39, 61, 81, 27, 24, 34, 55, 57, 83, 43, 55, 19, 63, 63, 66, 14, 39, 36, 57, 72, 26, 50, 44, 80, 77, 46, 87, 25, 17, 84, 29, 42, 52, 76, 26, 26, 44, 67, 10, 46, 68, 81, 74, 83, 29, 47, 17, 18, 24, 85, 79, 74, 81, 27, 26, 61, 21, 12, 64, 40, 54, 77, 67, 32, 20, 80, 53, 71, 13, 52, 30, 99]\n",
      "parallel [37, 30, 41, 69, 26, 68, 90, 17, 73, 77, 33, 79, 16, 80, 70, 54, 73, 56, 68, 28, 49, 42, 22, 63, 36, 21, 52, 57, 70, 46, 29, 54, 17, 74, 18, 20, 13, 25, 20, 59, 31, 80, 23, 40, 70, 24, 38, 55, 39, 61, 81, 27, 24, 34, 55, 57, 83, 43, 55, 19, 63, 63, 66, 14, 39, 36, 57, 72, 26, 50, 44, 80, 77, 46, 87, 25, 17, 84, 29, 42, 52, 76, 26, 26, 44, 67, 10, 46, 68, 81, 74, 83, 29, 47, 17, 18, 24, 85, 79, 74, 81, 27, 26, 61, 21, 12, 64, 40, 54, 77, 67, 32, 20, 80, 53, 71, 13, 52, 30, 99]\n",
      "List_output [{'correlation_std': np.float64(0.4652650691620318), 'maxed_goal_value': np.float64(0.14872313444134286), 'goal_base': np.float64(0.09311764305723447), 'maxed_proxy_value': np.float64(0.2542678544947297), 'correlation_on_max_proxy': np.float64(0.4022293350941809), 'correlation_base': np.float64(0.7536524368169873), 'optimal_goal_value': np.float64(0.12102769099081014), 'optimal_proxy_value': np.float64(0.4459820026426703), 'correlation_on_optimal_proxy': np.float64(0.34787102318456503), 'bin_indices': [37, 30, 41, 69, 26, 68, 90, 17, 73, 77, 33, 79, 16, 80, 70, 54, 73, 56, 68, 28, 49, 42, 22, 63, 36, 21, 52, 57, 70, 46, 29, 54, 17, 74, 18, 20, 13, 25, 20, 59, 31, 80, 23, 40, 70, 24, 38, 55, 39, 61, 81, 27, 24, 34, 55, 57, 83, 43, 55, 19, 63, 63, 66, 14, 39, 36, 57, 72, 26, 50, 44, 80, 77, 46, 87, 25, 17, 84, 29, 42, 52, 76, 26, 26, 44, 67, 10, 46, 68, 81, 74, 83, 29, 47, 17, 18, 24, 85, 79, 74, 81, 27, 26, 61, 21, 12, 64, 40, 54, 77, 67, 32, 20, 80, 53, 71, 13, 52, 30, 99], 'proxy_base': np.float64(0.09626462413084477)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "use_baby_params = False\n",
    "param_grid = {\n",
    "    'n': [120, 256, 512, 1024, 2048], \n",
    "    'trials': [50],\n",
    "    'proxy_discard': [50],\n",
    "    'goal_discard': [50, 150, 250, 450],\n",
    "    'measure_time': [50],\n",
    "    'spectral_radius': [1, 1.5, 2, 3],\n",
    "    'alpha': [0.1],\n",
    "    \"avg_number_of_edges\": [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "if use_baby_params:\n",
    "    param_grid = {\n",
    "        'n': [40], \n",
    "        'trials': [50],\n",
    "        'proxy_discard': [50],\n",
    "        'goal_discard': [100],\n",
    "        'measure_time': [50],\n",
    "        'spectral_radius': [1.5],\n",
    "        'alpha': [0.1],\n",
    "        \"avg_number_of_edges\": [1]\n",
    "    }\n",
    "\n",
    "\n",
    "generate_experimental_data('data_file_dump', param_grid, 1, intention = 'w', skip_to = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a04498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_combinations 320\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n': [120, 256, 512, 1024, 2048], \n",
    "    'trials': [50],\n",
    "    'proxy_discard': [50],\n",
    "    'goal_discard': [50, 150, 250, 450],\n",
    "    'measure_time': [50],\n",
    "    'spectral_radius': [1, 1.5, 2, 3],\n",
    "    'alpha': [0.1],\n",
    "    \"avg_number_of_edges\": [1, 2, 4, 8]\n",
    "}\n",
    "def reorder_experimental_data(filename, newfilename, param_grid, number_of_instances):\n",
    "    \n",
    "    # creates a list of the parameters that will vary\n",
    "    varying_params = [a for a in param_grid.keys() if len(param_grid[a]) > 1]\n",
    "    # Create a list of all combinations of parameters\n",
    "\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    print(\"param_combinations\", len(param_combinations))\n",
    "    # Create a list of parameter names\n",
    "    param_names = list(param_grid.keys())\n",
    "    # Loop through all combinations of parameters\n",
    "\n",
    "    fd = open(filename, \"r\")\n",
    "    reader = csv.reader(fd, delimiter = ',')\n",
    "    fw = open(newfilename, \"w\")\n",
    "    writer = csv.writer(fw, delimiter=',')\n",
    "    \n",
    "    fieldnames = ['correlation_std', 'maxed_goal_value','goal_base','maxed_proxy_value',\n",
    "                    'correlation_on_max_proxy','correlation_base','optimal_goal_value','optimal_proxy_value','correlation_on_optimal_proxy','proxy_base'] \n",
    "        \n",
    "    counter = 0\n",
    "    writer.writerow(param_names + fieldnames)\n",
    "    for line in reader:\n",
    "        if line != []:\n",
    "            counter+= 1\n",
    "            params = param_combinations[counter // number_of_instances]\n",
    "            del line[9]\n",
    "            writer.writerow(list(params)+ line )\n",
    "    fd.close()\n",
    "    fw.close()\n",
    "    \n",
    "\n",
    "        \n",
    "reorder_experimental_data('data_file',\"filtered_data_file.csv\", param_grid, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b30e427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for binning with numpy: 0.006895780563354492 seconds\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10097, 20221, 20259, 20275, 20381, 20295, 19951, 20140, 20222, 20104, 20236, 20302, 20208, 20485, 20181, 20258, 20283, 20058, 20544, 19874, 20143, 20210, 20154, 20051, 20366, 20088, 19957, 20051, 20283, 20144, 20322, 19894, 20217, 20233, 20288, 20056, 20378, 20101, 20218, 20082, 20135, 20364, 20061, 20209, 20182, 20319, 20325, 20334, 20223, 20238]\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "data = np.random.rand(1000000)  # Example correlation data for demonstration\n",
    "bins = np.linspace(-1, 1, 100)\n",
    "\n",
    "\n",
    "\n",
    "time_init  = time.time()\n",
    "bin_nb = np.histogram(data, bins)[0] \n",
    "bin_nb= [int(k) for k in bin_nb]\n",
    "time_end = time.time()\n",
    "print(\"Time taken for binning with numpy:\", time_end - time_init, \"seconds\")\n",
    "print(bin_nb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
